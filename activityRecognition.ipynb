{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for Human Activity Recognition Time Series Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the steps as shown in the following website:<br>\n",
    "https://machinelearningmastery.com/how-to-develop-lstm-models-for-human-activity-recognition-time-series-classification/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A group volunteers are asked to perform six (6) common human activities:\n",
    "1. Walking\n",
    "2. Walking Upstairs\n",
    "3. Walking Downstairs\n",
    "4. Sitting\n",
    "5. Standing\n",
    "6. Laying"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smartphone is attached to their waist and they are asked to perform the activities while wearing the smartphone. The smartphone has an embedded accelerometer and gyroscope. The accelerometer captures 3-axial linear acceleration and the gyroscope captures 3-axial angular velocity at a constant rate of 50Hz. With the provided data, I will train a LSTM model to predict the activity of the volunteer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import dstack, mean, std\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, TimeDistributed, Conv1D, MaxPooling1D, Flatten, ConvLSTM2D\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTM Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section is divided into the following sections:\n",
    "1. Load the Data\n",
    "2. Fit and Evaluate a Model\n",
    "3. Summarize Results\n",
    "4. Final Examination"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "  dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "  return dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''):\n",
    "  loaded = list()\n",
    "  for name in filenames:\n",
    "    data = load_file(prefix + name)\n",
    "    loaded.append(data)\n",
    "  # stack group so that features are the 3rd dimension\n",
    "  loaded = dstack(loaded)\n",
    "  return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "\tfilepath = prefix + group + '/Inertial Signals/'\n",
    "\t# load all 9 files as a single array\n",
    "\tfilenames = list()\n",
    "\t# total acceleration\n",
    "\tfilenames += ['total_acc_x_'+group+'.txt',\n",
    "               'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "\t# body acceleration\n",
    "\tfilenames += ['body_acc_x_'+group+'.txt',\n",
    "               'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['body_gyro_x_'+group+'.txt',\n",
    "               'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\t# load input data\n",
    "\tX = load_group(filenames, filepath)\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "\t# load all train\n",
    "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "\tprint(trainX.shape, trainy.shape)\n",
    "\t# load all test\n",
    "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "\tprint(testX.shape, testy.shape)\n",
    "\t# zero-offset class values\n",
    "\ttrainy = trainy - 1\n",
    "\ttesty = testy - 1\n",
    "\t# one hot encode y\n",
    "\ttrainy = to_categorical(trainy)\n",
    "\ttesty = to_categorical(testy)\n",
    "\tprint(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "\treturn trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Evaluate a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "  verbose, epochs, batch_size = 0, 15, 64\n",
    "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "  model = keras.Sequential([\n",
    "    LSTM(100, input_shape=(n_timesteps,n_features)),\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(n_outputs, activation='softmax')\n",
    "  ])\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  # fit network\n",
    "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "  # evaluate model\n",
    "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "  return accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "  print(scores)\n",
    "  m, s = mean(scores), std(scores)\n",
    "  print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an experiment\n",
    "def run_experiment(repeats=10):\n",
    "  # load data\n",
    "  trainX, trainy, testX, testy = load_dataset()\n",
    "  # repeat experiment\n",
    "  scores = list()\n",
    "  for r in range(repeats):\n",
    "    score = evaluate_model(trainX, trainy, testX, testy)\n",
    "    score = score * 100.0\n",
    "    print('>#%d: %.3f' % (r+1, score))\n",
    "    scores.append(score)\n",
    "  # summarize results\n",
    "  summarize_results(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
      ">#1: 91.517\n",
      ">#2: 88.056\n",
      ">#3: 89.345\n",
      ">#4: 89.820\n",
      ">#5: 90.431\n",
      ">#6: 86.291\n",
      ">#7: 90.499\n",
      ">#8: 90.329\n",
      ">#9: 90.533\n",
      ">#10: 90.770\n",
      "[91.51679873466492, 88.05565237998962, 89.34509754180908, 89.8201584815979, 90.43094515800476, 86.29114627838135, 90.49881100654602, 90.32914638519287, 90.53274393081665, 90.77027440071106]\n",
      "Accuracy: 89.759% (+/-1.454)\n"
     ]
    }
   ],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "  # define model\n",
    "  verbose, epochs, batch_size = 0, 25, 64\n",
    "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "  # reshape data into time steps of sub-sequences\n",
    "  n_steps, n_length = 4, 32\n",
    "  trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "  testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "  # define model\n",
    "  model = keras.Sequential([\n",
    "    TimeDistributed(Conv1D(filters=64, kernel_size=3,\n",
    "              activation='relu'), input_shape=(None, n_length, n_features)),\n",
    "    TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')),\n",
    "    TimeDistributed(Dropout(0.5)),\n",
    "    TimeDistributed(MaxPooling1D(pool_size=2)),\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(100),\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(n_outputs, activation='softmax')\n",
    "  ])\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "  # fit network\n",
    "  model.fit(trainX, trainy, epochs=epochs,\n",
    "            batch_size=batch_size, verbose=verbose)\n",
    "  # evaluate model\n",
    "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "  return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
      ">#1: 90.227\n",
      ">#2: 90.092\n",
      ">#3: 90.159\n",
      ">#4: 90.906\n",
      ">#5: 89.786\n",
      ">#6: 87.411\n",
      ">#7: 90.193\n",
      ">#8: 90.567\n",
      ">#9: 88.700\n",
      ">#10: 86.698\n",
      "[90.22734761238098, 90.09161591529846, 90.15948176383972, 90.90600609779358, 89.78622555732727, 87.41092681884766, 90.19341468811035, 90.56667685508728, 88.70037198066711, 86.69833540916443]\n",
      "Accuracy: 89.474% (+/-1.336)\n"
     ]
    }
   ],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLSTM Network Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further extension of the CNN LSTM idea is to perform the convolutions of the CNN (e.g. how the CNN reads the input sequence data) as part of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "  # define model\n",
    "  verbose, epochs, batch_size = 0, 25, 64\n",
    "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "  # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "  n_steps, n_length = 4, 32\n",
    "  trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "  testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "  # define model\n",
    "  model = keras.Sequential([\n",
    "    ConvLSTM2D(filters=64, kernel_size=(1, 3),\n",
    "              activation='relu', input_shape=(n_steps, 1, n_length, n_features)),\n",
    "    Dropout(0.5),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(n_outputs, activation='softmax')\n",
    "  ])\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "  # fit network\n",
    "  model.fit(trainX, trainy, epochs=epochs,\n",
    "            batch_size=batch_size, verbose=verbose)\n",
    "  # evaluate model\n",
    "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
      ">#1: 91.890\n",
      ">#2: 90.635\n",
      ">#3: 91.042\n",
      ">#4: 90.193\n",
      ">#5: 90.567\n",
      ">#6: 89.990\n",
      ">#7: 88.768\n",
      ">#8: 89.379\n",
      ">#9: 89.108\n",
      ">#10: 90.601\n",
      "[91.89005494117737, 90.63454270362854, 91.0417377948761, 90.19341468811035, 90.56667685508728, 89.98982310295105, 88.76823782920837, 89.37903046607971, 89.10756707191467, 90.60060977935791]\n",
      "Accuracy: 90.217% (+/-0.895)\n"
     ]
    }
   ],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License:\n",
    "# ========\n",
    "# Use of this dataset in publications must be acknowledged by referencing the following publication [1] \n",
    "\n",
    "# [1] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz.\n",
    "# A Public Domain Dataset for Human Activity Recognition Using Smartphones.\n",
    "# 21th European Symposium on Artificial Neural Networks, Computational Intelligence\n",
    "# and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013. \n",
    "\n",
    "# This dataset is distributed AS-IS and no responsibility implied or\n",
    "# explicit can be addressed to the authors or their institutions for\n",
    "# its use or misuse. Any commercial use is prohibited.\n",
    "\n",
    "# Other Related Publications:\n",
    "# ===========================\n",
    "# [2] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge L.\n",
    "# Reyes-Ortiz.  Energy Efficient Smartphone-Based Activity Recognition\n",
    "# using Fixed-Point Arithmetic. Journal of Universal Computer Science.\n",
    "# Special Issue in Ambient Assisted Living: Home Care.   Volume 19, Issue 9. May 2013\n",
    "\n",
    "# [3] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and\n",
    "# Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones\n",
    "# using a Multiclass Hardware-Friendly Support Vector Machine. 4th\n",
    "# International Workshop of Ambient Assited Living, IWAAL 2012,\n",
    "# Vitoria-Gasteiz, Spain, December 3-5, 2012. Proceedings. Lecture\n",
    "# Notes in Computer Science 2012, pp 216-223. \n",
    "\n",
    "# [4] Jorge Luis Reyes-Ortiz, Alessandro Ghio, Xavier Parra-Llanas,\n",
    "# Davide Anguita, Joan Cabestany, Andreu Català. Human Activity and\n",
    "# Motion Disorder Recognition: Towards Smarter Interactive Cognitive\n",
    "# Environments. 21th European Symposium on Artificial Neural Networks,\n",
    "# Computational Intelligence and Machine Learning, ESANN 2013. Bruges,\n",
    "# Belgium 24-26 April 2013.  \n",
    "\n",
    "# ==================================================================================================\n",
    "# Jorge L. Reyes-Ortiz, Alessandro Ghio, Luca Oneto, Davide Anguita and Xavier Parra. November 2013."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1534846e4fcc353f8c5feb22b2d4f8b6e6f8be66008bac3fccdeb2b473060024"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
